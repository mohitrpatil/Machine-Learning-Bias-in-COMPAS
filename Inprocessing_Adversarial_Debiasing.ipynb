{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP3IRmp6Yy1D",
        "outputId": "1a44e97f-2955-423a-ee23-1de52e1350f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting aif360\n",
            "  Downloading aif360-0.5.0-py3-none-any.whl (214 kB)\n",
            "\u001b[K     |████████████████████████████████| 214 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from aif360) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from aif360) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.8/dist-packages (from aif360) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.8/dist-packages (from aif360) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from aif360) (1.7.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->aif360) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->aif360) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0->aif360) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0->aif360) (3.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->aif360) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->aif360) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->aif360) (3.0.9)\n",
            "Installing collected packages: aif360\n",
            "Successfully installed aif360-0.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\n",
            "pip install 'aif360[LawSchoolGPA]'\n",
            "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
            "pip install 'aif360[Reductions]'\n",
            "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
            "pip install 'aif360[Reductions]'\n",
            "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
            "pip install 'aif360[Reductions]'\n"
          ]
        }
      ],
      "source": [
        "!pip install aif360\n",
        "%matplotlib inline\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"../\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.datasets import CompasDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.metrics import ClassificationMetric\n",
        "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_compas\n",
        "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "original = r'/content/compas-scores-two-years.csv'\n",
        "target = r'/usr/local/lib/python3.8/dist-packages/aif360/data/raw/compas'\n",
        "\n",
        "shutil.copy(original,target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-b1SAbfMY3th",
        "outputId": "25a8a359-e97e-4b6b-bd5a-f7459f5ad097"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.8/dist-packages/aif360/data/raw/compas/compas-scores-two-years.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preproc_data_compas_data(protected_attributes):\n",
        "    def preprocessing(df):\n",
        "        df = df[['age', 'c_charge_degree', 'race', 'age_cat', 'score_text','sex', 'priors_count', 'days_b_screening_arrest', 'decile_score',\n",
        "                 'is_recid', 'two_year_recid', 'c_jail_in', 'c_jail_out']]\n",
        "        \n",
        "        features = ['two_year_recid','sex', 'race','age_cat', 'priors_count', 'c_charge_degree']\n",
        "\n",
        "        def quantizeLOS(x):\n",
        "            if x<= 7:\n",
        "                return '<week'\n",
        "            if 8<x<=93:\n",
        "                return '<3months'\n",
        "            else:\n",
        "                return '>3 months'\n",
        "\n",
        "        def quantizePrior(x):\n",
        "            if x <=0:\n",
        "                return '0'\n",
        "            elif 1<=x<=3:\n",
        "                return '1 to 3'\n",
        "            else:\n",
        "                return 'More than 3'\n",
        "\n",
        "        def quantizeScore(x):\n",
        "            if (x == 'High')| (x == 'Medium'):\n",
        "                return 'MediumHigh'\n",
        "            else:\n",
        "                return x\n",
        "\n",
        "        def group_race(x):\n",
        "            if x == \"Caucasian\":\n",
        "                return 1.0\n",
        "            else:\n",
        "                return 0.0\n",
        "\n",
        "        def adjustAge(x):\n",
        "            if x == '25 - 45':\n",
        "                return '25 to 45'\n",
        "            else:\n",
        "                return x\n",
        "\n",
        "        indx = df['days_b_screening_arrest'] <= 30\n",
        "        indx = (df['is_recid'] != -1) & indx\n",
        "        indx = (df['days_b_screening_arrest'] >= -30) & indx\n",
        "        indx = (df['score_text'] != 'N/A') & indx\n",
        "        indx = (df['c_charge_degree'] != \"O\") & indx\n",
        "        \n",
        "        df = df.loc[indx,:]\n",
        "        df['length_of_stay'] = (pd.to_datetime(df['c_jail_out'])- pd.to_datetime(df['c_jail_in'])) / np.timedelta64(1, 'D')\n",
        "        \n",
        "        df_filter = df.loc[~df['race'].isin(['Native American','Hispanic','Asian','Other']),:]\n",
        "\n",
        "        df_filter_fields = df_filter[['sex','race','age_cat','c_charge_degree','score_text','priors_count','is_recid','two_year_recid','length_of_stay']].copy()\n",
        "\n",
        "        df_filter_fields['priors_count'] = df_filter_fields['priors_count'].apply(lambda x: quantizePrior(x))\n",
        "        df_filter_fields['length_of_stay'] = df_filter_fields['length_of_stay'].apply(lambda x: quantizeLOS(x))\n",
        "        df_filter_fields['score_text'] = df_filter_fields['score_text'].apply(lambda x: quantizeScore(x))\n",
        "        df_filter_fields['age_cat'] = df_filter_fields['age_cat'].apply(lambda x: adjustAge(x))\n",
        "\n",
        "        df_filter_fields['sex'] = df_filter_fields['sex'].replace({'Female': 1.0, 'Male': 0.0})\n",
        "        df_filter_fields['race'] = df_filter_fields['race'].apply(lambda x: group_race(x))\n",
        "        df = df_filter_fields[features]\n",
        "        return df\n",
        "\n",
        "    list_features = ['age_cat', 'c_charge_degree', 'priors_count', 'sex', 'race']\n",
        "    D_features = [protected_attributes]  if protected_attributes is None else protected_attributes\n",
        "    Y_features = ['two_year_recid']\n",
        "    X_features = list(set(list_features)-set(D_features))\n",
        "    category_features = ['age_cat', 'priors_count', 'c_charge_degree']\n",
        "\n",
        "    all_privileged_classes = {\"sex\": [1.0],\n",
        "                              \"race\": [1.0]}\n",
        "\n",
        "    all_bias_maps = {\"sex\": {0.0: 'Male', 1.0: 'Female'},\n",
        "                                    \"race\": {1.0: 'Caucasian', 0.0: 'Not Caucasian'}}\n",
        "\n",
        "\n",
        "    return CompasDataset(\n",
        "        label_name=Y_features[0],\n",
        "        favorable_classes=[0],\n",
        "        protected_attribute_names=D_features,\n",
        "        privileged_classes=[all_privileged_classes[x] for x in D_features],\n",
        "        instance_weights_name=None,\n",
        "        categorical_features=category_features,\n",
        "        features_to_keep=X_features+Y_features+D_features,\n",
        "        na_values=[],\n",
        "        metadata={'label_maps': [{1.0: 'Did recid.', 0.0: 'No recid.'}],\n",
        "                  'protected_attribute_maps': [all_bias_maps[x]\n",
        "                                for x in D_features]},\n",
        "        custom_preprocessing=preprocessing)\n"
      ],
      "metadata": {
        "id": "UCF-o1R4wuo6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYAvFQNmYy1F"
      },
      "source": [
        "#### Load dataset and set options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P8EjdQ2nYy1F"
      },
      "outputs": [],
      "source": [
        "orig_data = load_preproc_data_compas_data(['race'])\n",
        "privileged = [{'race': 1}]\n",
        "non_privileged = [{'race': 0}]\n",
        "orig_train_set, orig_test_set = orig_data.split([0.7], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2dBWlY0Yy1G",
        "outputId": "eedbb47e-b39e-46dc-8ca9-2fa6c2fd96ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset shape:  (3694, 10)\n",
            "Favorable and unfavorable labels:  0.0 1.0\n",
            "Protected attribute names:  ['race']\n",
            "Dataset feature names: ['sex', 'race', 'age_cat=25 to 45', 'age_cat=Greater than 45', 'age_cat=Less than 25', 'priors_count=0', 'priors_count=1 to 3', 'priors_count=More than 3', 'c_charge_degree=F', 'c_charge_degree=M']\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Dataset shape: \", orig_train_set.features.shape  )\n",
        "print(\"Favorable and unfavorable labels: \",orig_train_set.favorable_label, orig_train_set.unfavorable_label)\n",
        "print(\"Protected attribute names: \", orig_train_set.protected_attribute_names)\n",
        "print(\"Dataset feature names:\", orig_train_set.feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2AZOVMqYy1H"
      },
      "source": [
        "#### Metric for original training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnGV3MocYy1H",
        "outputId": "edcc29ea-8423-4e50-9f40-1b094909af47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training dataset...\n",
            "Train set: Difference in mean outcomes between unprivileged and privileged groups =  -0.1296\n",
            "Test set: Difference in mean outcomes between unprivileged and privileged groups =  -0.1389\n"
          ]
        }
      ],
      "source": [
        "orig_train_metric = BinaryLabelDatasetMetric(orig_train_set, \n",
        "                                             unprivileged_groups=non_privileged,\n",
        "                                             privileged_groups=privileged)\n",
        "print(\"Original training dataset...\")\n",
        "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = \", float(round(orig_train_metric.mean_difference(),4)))\n",
        "orig_test_metric = BinaryLabelDatasetMetric(orig_test_set, \n",
        "                                             unprivileged_groups=non_privileged,\n",
        "                                             privileged_groups=privileged)\n",
        "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = \", float(round(orig_test_metric.mean_difference(),4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siDcgu6XYy1J",
        "outputId": "3842eb13-f7ee-4874-ae6d-e6ef4891f863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled dataset - Verify that the scaling does not affect the group label statistics\n",
            "Train set: Difference in mean outcomes between unprivileged and privileged groups =  -0.1296\n",
            "Test set: Difference in mean outcomes between unprivileged and privileged groups =  -0.1389\n"
          ]
        }
      ],
      "source": [
        "scaling = MaxAbsScaler()\n",
        "orig_train_set.features = scaling.fit_transform(orig_train_set.features)\n",
        "orig_test_set.features = scaling.transform(orig_test_set.features)\n",
        "scaled_train_metric = BinaryLabelDatasetMetric(orig_train_set, \n",
        "                             unprivileged_groups=non_privileged,\n",
        "                             privileged_groups=privileged)\n",
        "\n",
        "print(\"Scaled dataset - Verify that the scaling does not affect the group label statistics\")\n",
        "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = \", float(round(scaled_train_metric.mean_difference(),4)))\n",
        "scaled_test_metric = BinaryLabelDatasetMetric(orig_test_set, \n",
        "                             unprivileged_groups=non_privileged,\n",
        "                             privileged_groups=privileged)\n",
        "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = \",float(round(scaled_test_metric.mean_difference(),4)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtRg15yYYy1J"
      },
      "source": [
        "### Learn plan classifier without debiasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dWUZRunvYy1K"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "sess = tf.Session()\n",
        "\n",
        "model_used = AdversarialDebiasing(privileged_groups = privileged,\n",
        "                          unprivileged_groups = non_privileged,\n",
        "                          scope_name='plain_classifier',\n",
        "                          debias=False,\n",
        "                          sess=sess,\n",
        "                          num_epochs = 200,\n",
        "                          batch_size = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eraF0tEzYy1K",
        "outputId": "10276964-38cf-4013-8de7-02558d09d732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0; iter: 0; batch classifier loss: 0.698201\n",
            "epoch 1; iter: 0; batch classifier loss: 0.650327\n",
            "epoch 2; iter: 0; batch classifier loss: 0.598011\n",
            "epoch 3; iter: 0; batch classifier loss: 0.633954\n",
            "epoch 4; iter: 0; batch classifier loss: 0.644100\n",
            "epoch 5; iter: 0; batch classifier loss: 0.689132\n",
            "epoch 6; iter: 0; batch classifier loss: 0.630500\n",
            "epoch 7; iter: 0; batch classifier loss: 0.598679\n",
            "epoch 8; iter: 0; batch classifier loss: 0.655322\n",
            "epoch 9; iter: 0; batch classifier loss: 0.699942\n",
            "epoch 10; iter: 0; batch classifier loss: 0.680058\n",
            "epoch 11; iter: 0; batch classifier loss: 0.604512\n",
            "epoch 12; iter: 0; batch classifier loss: 0.621667\n",
            "epoch 13; iter: 0; batch classifier loss: 0.613402\n",
            "epoch 14; iter: 0; batch classifier loss: 0.619868\n",
            "epoch 15; iter: 0; batch classifier loss: 0.642618\n",
            "epoch 16; iter: 0; batch classifier loss: 0.626561\n",
            "epoch 17; iter: 0; batch classifier loss: 0.615074\n",
            "epoch 18; iter: 0; batch classifier loss: 0.616662\n",
            "epoch 19; iter: 0; batch classifier loss: 0.597395\n",
            "epoch 20; iter: 0; batch classifier loss: 0.611860\n",
            "epoch 21; iter: 0; batch classifier loss: 0.587296\n",
            "epoch 22; iter: 0; batch classifier loss: 0.601236\n",
            "epoch 23; iter: 0; batch classifier loss: 0.613701\n",
            "epoch 24; iter: 0; batch classifier loss: 0.643540\n",
            "epoch 25; iter: 0; batch classifier loss: 0.624997\n",
            "epoch 26; iter: 0; batch classifier loss: 0.553308\n",
            "epoch 27; iter: 0; batch classifier loss: 0.635197\n",
            "epoch 28; iter: 0; batch classifier loss: 0.562701\n",
            "epoch 29; iter: 0; batch classifier loss: 0.646680\n",
            "epoch 30; iter: 0; batch classifier loss: 0.623543\n",
            "epoch 31; iter: 0; batch classifier loss: 0.577625\n",
            "epoch 32; iter: 0; batch classifier loss: 0.545351\n",
            "epoch 33; iter: 0; batch classifier loss: 0.668258\n",
            "epoch 34; iter: 0; batch classifier loss: 0.603862\n",
            "epoch 35; iter: 0; batch classifier loss: 0.563642\n",
            "epoch 36; iter: 0; batch classifier loss: 0.623701\n",
            "epoch 37; iter: 0; batch classifier loss: 0.687624\n",
            "epoch 38; iter: 0; batch classifier loss: 0.639147\n",
            "epoch 39; iter: 0; batch classifier loss: 0.648791\n",
            "epoch 40; iter: 0; batch classifier loss: 0.651494\n",
            "epoch 41; iter: 0; batch classifier loss: 0.652474\n",
            "epoch 42; iter: 0; batch classifier loss: 0.588882\n",
            "epoch 43; iter: 0; batch classifier loss: 0.620661\n",
            "epoch 44; iter: 0; batch classifier loss: 0.607753\n",
            "epoch 45; iter: 0; batch classifier loss: 0.644340\n",
            "epoch 46; iter: 0; batch classifier loss: 0.650281\n",
            "epoch 47; iter: 0; batch classifier loss: 0.577328\n",
            "epoch 48; iter: 0; batch classifier loss: 0.588497\n",
            "epoch 49; iter: 0; batch classifier loss: 0.606889\n",
            "epoch 50; iter: 0; batch classifier loss: 0.627450\n",
            "epoch 51; iter: 0; batch classifier loss: 0.632467\n",
            "epoch 52; iter: 0; batch classifier loss: 0.600027\n",
            "epoch 53; iter: 0; batch classifier loss: 0.646479\n",
            "epoch 54; iter: 0; batch classifier loss: 0.621318\n",
            "epoch 55; iter: 0; batch classifier loss: 0.575985\n",
            "epoch 56; iter: 0; batch classifier loss: 0.601891\n",
            "epoch 57; iter: 0; batch classifier loss: 0.568194\n",
            "epoch 58; iter: 0; batch classifier loss: 0.597211\n",
            "epoch 59; iter: 0; batch classifier loss: 0.625342\n",
            "epoch 60; iter: 0; batch classifier loss: 0.575918\n",
            "epoch 61; iter: 0; batch classifier loss: 0.619225\n",
            "epoch 62; iter: 0; batch classifier loss: 0.556414\n",
            "epoch 63; iter: 0; batch classifier loss: 0.649291\n",
            "epoch 64; iter: 0; batch classifier loss: 0.628925\n",
            "epoch 65; iter: 0; batch classifier loss: 0.622924\n",
            "epoch 66; iter: 0; batch classifier loss: 0.605505\n",
            "epoch 67; iter: 0; batch classifier loss: 0.589866\n",
            "epoch 68; iter: 0; batch classifier loss: 0.587631\n",
            "epoch 69; iter: 0; batch classifier loss: 0.627228\n",
            "epoch 70; iter: 0; batch classifier loss: 0.541439\n",
            "epoch 71; iter: 0; batch classifier loss: 0.613389\n",
            "epoch 72; iter: 0; batch classifier loss: 0.639153\n",
            "epoch 73; iter: 0; batch classifier loss: 0.599423\n",
            "epoch 74; iter: 0; batch classifier loss: 0.650792\n",
            "epoch 75; iter: 0; batch classifier loss: 0.585565\n",
            "epoch 76; iter: 0; batch classifier loss: 0.634284\n",
            "epoch 77; iter: 0; batch classifier loss: 0.610079\n",
            "epoch 78; iter: 0; batch classifier loss: 0.601901\n",
            "epoch 79; iter: 0; batch classifier loss: 0.603162\n",
            "epoch 80; iter: 0; batch classifier loss: 0.608307\n",
            "epoch 81; iter: 0; batch classifier loss: 0.623143\n",
            "epoch 82; iter: 0; batch classifier loss: 0.636175\n",
            "epoch 83; iter: 0; batch classifier loss: 0.630247\n",
            "epoch 84; iter: 0; batch classifier loss: 0.664863\n",
            "epoch 85; iter: 0; batch classifier loss: 0.627284\n",
            "epoch 86; iter: 0; batch classifier loss: 0.635510\n",
            "epoch 87; iter: 0; batch classifier loss: 0.569938\n",
            "epoch 88; iter: 0; batch classifier loss: 0.646835\n",
            "epoch 89; iter: 0; batch classifier loss: 0.647547\n",
            "epoch 90; iter: 0; batch classifier loss: 0.593862\n",
            "epoch 91; iter: 0; batch classifier loss: 0.543502\n",
            "epoch 92; iter: 0; batch classifier loss: 0.621117\n",
            "epoch 93; iter: 0; batch classifier loss: 0.647479\n",
            "epoch 94; iter: 0; batch classifier loss: 0.590200\n",
            "epoch 95; iter: 0; batch classifier loss: 0.577054\n",
            "epoch 96; iter: 0; batch classifier loss: 0.591180\n",
            "epoch 97; iter: 0; batch classifier loss: 0.639927\n",
            "epoch 98; iter: 0; batch classifier loss: 0.606249\n",
            "epoch 99; iter: 0; batch classifier loss: 0.622546\n",
            "epoch 100; iter: 0; batch classifier loss: 0.602348\n",
            "epoch 101; iter: 0; batch classifier loss: 0.634283\n",
            "epoch 102; iter: 0; batch classifier loss: 0.638960\n",
            "epoch 103; iter: 0; batch classifier loss: 0.654408\n",
            "epoch 104; iter: 0; batch classifier loss: 0.615569\n",
            "epoch 105; iter: 0; batch classifier loss: 0.606321\n",
            "epoch 106; iter: 0; batch classifier loss: 0.602229\n",
            "epoch 107; iter: 0; batch classifier loss: 0.674052\n",
            "epoch 108; iter: 0; batch classifier loss: 0.681223\n",
            "epoch 109; iter: 0; batch classifier loss: 0.586457\n",
            "epoch 110; iter: 0; batch classifier loss: 0.633066\n",
            "epoch 111; iter: 0; batch classifier loss: 0.611944\n",
            "epoch 112; iter: 0; batch classifier loss: 0.658980\n",
            "epoch 113; iter: 0; batch classifier loss: 0.661002\n",
            "epoch 114; iter: 0; batch classifier loss: 0.590363\n",
            "epoch 115; iter: 0; batch classifier loss: 0.593561\n",
            "epoch 116; iter: 0; batch classifier loss: 0.644549\n",
            "epoch 117; iter: 0; batch classifier loss: 0.598258\n",
            "epoch 118; iter: 0; batch classifier loss: 0.640700\n",
            "epoch 119; iter: 0; batch classifier loss: 0.675680\n",
            "epoch 120; iter: 0; batch classifier loss: 0.613221\n",
            "epoch 121; iter: 0; batch classifier loss: 0.684283\n",
            "epoch 122; iter: 0; batch classifier loss: 0.640872\n",
            "epoch 123; iter: 0; batch classifier loss: 0.706202\n",
            "epoch 124; iter: 0; batch classifier loss: 0.611955\n",
            "epoch 125; iter: 0; batch classifier loss: 0.673988\n",
            "epoch 126; iter: 0; batch classifier loss: 0.553491\n",
            "epoch 127; iter: 0; batch classifier loss: 0.591530\n",
            "epoch 128; iter: 0; batch classifier loss: 0.620782\n",
            "epoch 129; iter: 0; batch classifier loss: 0.612194\n",
            "epoch 130; iter: 0; batch classifier loss: 0.537980\n",
            "epoch 131; iter: 0; batch classifier loss: 0.603226\n",
            "epoch 132; iter: 0; batch classifier loss: 0.666525\n",
            "epoch 133; iter: 0; batch classifier loss: 0.634308\n",
            "epoch 134; iter: 0; batch classifier loss: 0.636258\n",
            "epoch 135; iter: 0; batch classifier loss: 0.576118\n",
            "epoch 136; iter: 0; batch classifier loss: 0.631336\n",
            "epoch 137; iter: 0; batch classifier loss: 0.612103\n",
            "epoch 138; iter: 0; batch classifier loss: 0.563026\n",
            "epoch 139; iter: 0; batch classifier loss: 0.639562\n",
            "epoch 140; iter: 0; batch classifier loss: 0.561996\n",
            "epoch 141; iter: 0; batch classifier loss: 0.627445\n",
            "epoch 142; iter: 0; batch classifier loss: 0.575131\n",
            "epoch 143; iter: 0; batch classifier loss: 0.642414\n",
            "epoch 144; iter: 0; batch classifier loss: 0.604474\n",
            "epoch 145; iter: 0; batch classifier loss: 0.602222\n",
            "epoch 146; iter: 0; batch classifier loss: 0.604205\n",
            "epoch 147; iter: 0; batch classifier loss: 0.612969\n",
            "epoch 148; iter: 0; batch classifier loss: 0.649994\n",
            "epoch 149; iter: 0; batch classifier loss: 0.557277\n",
            "epoch 150; iter: 0; batch classifier loss: 0.645631\n",
            "epoch 151; iter: 0; batch classifier loss: 0.651921\n",
            "epoch 152; iter: 0; batch classifier loss: 0.641677\n",
            "epoch 153; iter: 0; batch classifier loss: 0.649093\n",
            "epoch 154; iter: 0; batch classifier loss: 0.628988\n",
            "epoch 155; iter: 0; batch classifier loss: 0.659302\n",
            "epoch 156; iter: 0; batch classifier loss: 0.638849\n",
            "epoch 157; iter: 0; batch classifier loss: 0.626060\n",
            "epoch 158; iter: 0; batch classifier loss: 0.586972\n",
            "epoch 159; iter: 0; batch classifier loss: 0.566891\n",
            "epoch 160; iter: 0; batch classifier loss: 0.573741\n",
            "epoch 161; iter: 0; batch classifier loss: 0.671007\n",
            "epoch 162; iter: 0; batch classifier loss: 0.605919\n",
            "epoch 163; iter: 0; batch classifier loss: 0.677293\n",
            "epoch 164; iter: 0; batch classifier loss: 0.648180\n",
            "epoch 165; iter: 0; batch classifier loss: 0.580121\n",
            "epoch 166; iter: 0; batch classifier loss: 0.605703\n",
            "epoch 167; iter: 0; batch classifier loss: 0.657106\n",
            "epoch 168; iter: 0; batch classifier loss: 0.678062\n",
            "epoch 169; iter: 0; batch classifier loss: 0.633515\n",
            "epoch 170; iter: 0; batch classifier loss: 0.627996\n",
            "epoch 171; iter: 0; batch classifier loss: 0.653613\n",
            "epoch 172; iter: 0; batch classifier loss: 0.626394\n",
            "epoch 173; iter: 0; batch classifier loss: 0.596634\n",
            "epoch 174; iter: 0; batch classifier loss: 0.615900\n",
            "epoch 175; iter: 0; batch classifier loss: 0.644889\n",
            "epoch 176; iter: 0; batch classifier loss: 0.627513\n",
            "epoch 177; iter: 0; batch classifier loss: 0.672519\n",
            "epoch 178; iter: 0; batch classifier loss: 0.588295\n",
            "epoch 179; iter: 0; batch classifier loss: 0.652645\n",
            "epoch 180; iter: 0; batch classifier loss: 0.686670\n",
            "epoch 181; iter: 0; batch classifier loss: 0.617986\n",
            "epoch 182; iter: 0; batch classifier loss: 0.638311\n",
            "epoch 183; iter: 0; batch classifier loss: 0.598501\n",
            "epoch 184; iter: 0; batch classifier loss: 0.627331\n",
            "epoch 185; iter: 0; batch classifier loss: 0.598141\n",
            "epoch 186; iter: 0; batch classifier loss: 0.591461\n",
            "epoch 187; iter: 0; batch classifier loss: 0.672528\n",
            "epoch 188; iter: 0; batch classifier loss: 0.635720\n",
            "epoch 189; iter: 0; batch classifier loss: 0.608042\n",
            "epoch 190; iter: 0; batch classifier loss: 0.605721\n",
            "epoch 191; iter: 0; batch classifier loss: 0.611844\n",
            "epoch 192; iter: 0; batch classifier loss: 0.576024\n",
            "epoch 193; iter: 0; batch classifier loss: 0.612878\n",
            "epoch 194; iter: 0; batch classifier loss: 0.636051\n",
            "epoch 195; iter: 0; batch classifier loss: 0.623925\n",
            "epoch 196; iter: 0; batch classifier loss: 0.612719\n",
            "epoch 197; iter: 0; batch classifier loss: 0.581326\n",
            "epoch 198; iter: 0; batch classifier loss: 0.659617\n",
            "epoch 199; iter: 0; batch classifier loss: 0.612012\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x7f8333ba8af0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model_used.fit(orig_train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Kqx3PxMNYy1L"
      },
      "outputs": [],
      "source": [
        "nodebiasing_train_data = model_used.predict(orig_train_set)\n",
        "nodebiasing_test_data = model_used.predict(orig_test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RKLgMmxLYy1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004551d2-9556-4674-c85b-48090e3a304d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plain model - without debiasing - dataset metrics\n",
            "Train set: Difference in mean outcomes between unprivileged and privileged groups =  -0.2748\n",
            "Test set: Difference in mean outcomes between unprivileged and privileged groups =  0.0\n",
            "\n",
            "Plain model - without debiasing - classification metrics\n",
            "Test set: Classification accuracy =  0.6812\n",
            "Test set: Balanced classification accuracy =  0.6741\n",
            "Test set: Disparate impact =  0.6565\n",
            "Test set: Equal opportunity difference =  -0.1795\n",
            "Test set: Average odds difference =  -0.2244\n",
            "Test set: Theil_index =  0.1862\n"
          ]
        }
      ],
      "source": [
        "nodebiasing_train_data_metric = BinaryLabelDatasetMetric(nodebiasing_train_data, \n",
        "                                             unprivileged_groups=non_privileged,\n",
        "                                             privileged_groups=privileged)\n",
        "nodebiasing_test_data_metric = BinaryLabelDatasetMetric(nodebiasing_test_data, \n",
        "                                             unprivileged_groups=non_privileged,\n",
        "                                             privileged_groups=privileged)\n",
        "print(\"Plain model - without debiasing - dataset metrics\")\n",
        "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = \", float(round(nodebiasing_train_data_metric.mean_difference(),4)))\n",
        "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = \", float(round(nodebiasing_test_data_metric.mean_difference())))\n",
        "\n",
        "nodebiasing_test_classify_metric = ClassificationMetric(orig_test_set, \n",
        "                                                 nodebiasing_test_data,\n",
        "                                                 unprivileged_groups=non_privileged,\n",
        "                                                 privileged_groups=privileged)\n",
        "print(\"\\nPlain model - without debiasing - classification metrics\")\n",
        "print(\"Test set: Classification accuracy = \", float(round(nodebiasing_test_classify_metric.accuracy(),4)))\n",
        "\n",
        "bal_acc_nodebiasing_test = 0.5*(nodebiasing_test_classify_metric.true_positive_rate()+nodebiasing_test_classify_metric.true_negative_rate())\n",
        "print(\"Test set: Balanced classification accuracy = \", float(round(bal_acc_nodebiasing_test,4)))\n",
        "print(\"Test set: Disparate impact = \", float(round(nodebiasing_test_classify_metric.disparate_impact(),4)))\n",
        "print(\"Test set: Equal opportunity difference = \", float(round(nodebiasing_test_classify_metric.equal_opportunity_difference(),4)))\n",
        "print(\"Test set: Average odds difference = \", float(round(nodebiasing_test_classify_metric.average_odds_difference(),4)))\n",
        "print(\"Test set: Theil_index = \", float(round(nodebiasing_test_classify_metric.theil_index(),4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psld4SFzYy1M"
      },
      "source": [
        "### Apply in-processing algorithm based on adversarial learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "emcNreMPYy1M"
      },
      "outputs": [],
      "source": [
        "sess.close()\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Nsg0bHUvYy1M"
      },
      "outputs": [],
      "source": [
        "debiased_model = AdversarialDebiasing(privileged_groups = privileged,\n",
        "                          unprivileged_groups = non_privileged,\n",
        "                          scope_name='debiased_classifier',\n",
        "                          debias=True,\n",
        "                          sess=sess,\n",
        "                          num_epochs=200,\n",
        "                          batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "scrolled": true,
        "id": "KEZv8alxYy1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9971c0b-c594-43c4-a8ad-eab8d8dd6a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0; iter: 0; batch classifier loss: 0.694927; batch adversarial loss: 0.772855\n",
            "epoch 1; iter: 0; batch classifier loss: 0.620671; batch adversarial loss: 0.786679\n",
            "epoch 2; iter: 0; batch classifier loss: 0.638709; batch adversarial loss: 0.861380\n",
            "epoch 3; iter: 0; batch classifier loss: 0.622501; batch adversarial loss: 0.799589\n",
            "epoch 4; iter: 0; batch classifier loss: 0.623318; batch adversarial loss: 0.874145\n",
            "epoch 5; iter: 0; batch classifier loss: 0.680665; batch adversarial loss: 0.953986\n",
            "epoch 6; iter: 0; batch classifier loss: 0.619786; batch adversarial loss: 0.908979\n",
            "epoch 7; iter: 0; batch classifier loss: 0.623734; batch adversarial loss: 0.851198\n",
            "epoch 8; iter: 0; batch classifier loss: 0.638068; batch adversarial loss: 0.803699\n",
            "epoch 9; iter: 0; batch classifier loss: 0.629252; batch adversarial loss: 0.854450\n",
            "epoch 10; iter: 0; batch classifier loss: 0.624048; batch adversarial loss: 0.837976\n",
            "epoch 11; iter: 0; batch classifier loss: 0.608223; batch adversarial loss: 0.860261\n",
            "epoch 12; iter: 0; batch classifier loss: 0.653375; batch adversarial loss: 0.892701\n",
            "epoch 13; iter: 0; batch classifier loss: 0.649624; batch adversarial loss: 0.815974\n",
            "epoch 14; iter: 0; batch classifier loss: 0.713508; batch adversarial loss: 0.816719\n",
            "epoch 15; iter: 0; batch classifier loss: 0.618322; batch adversarial loss: 0.797925\n",
            "epoch 16; iter: 0; batch classifier loss: 0.648850; batch adversarial loss: 0.824229\n",
            "epoch 17; iter: 0; batch classifier loss: 0.613111; batch adversarial loss: 0.815805\n",
            "epoch 18; iter: 0; batch classifier loss: 0.624977; batch adversarial loss: 0.774600\n",
            "epoch 19; iter: 0; batch classifier loss: 0.640255; batch adversarial loss: 0.800582\n",
            "epoch 20; iter: 0; batch classifier loss: 0.691017; batch adversarial loss: 0.781105\n",
            "epoch 21; iter: 0; batch classifier loss: 0.716498; batch adversarial loss: 0.792198\n",
            "epoch 22; iter: 0; batch classifier loss: 0.665434; batch adversarial loss: 0.768089\n",
            "epoch 23; iter: 0; batch classifier loss: 0.662542; batch adversarial loss: 0.761898\n",
            "epoch 24; iter: 0; batch classifier loss: 0.660820; batch adversarial loss: 0.766670\n",
            "epoch 25; iter: 0; batch classifier loss: 0.712154; batch adversarial loss: 0.761824\n",
            "epoch 26; iter: 0; batch classifier loss: 0.598102; batch adversarial loss: 0.765273\n",
            "epoch 27; iter: 0; batch classifier loss: 0.670018; batch adversarial loss: 0.727281\n",
            "epoch 28; iter: 0; batch classifier loss: 0.697483; batch adversarial loss: 0.720748\n",
            "epoch 29; iter: 0; batch classifier loss: 0.564193; batch adversarial loss: 0.769930\n",
            "epoch 30; iter: 0; batch classifier loss: 0.628979; batch adversarial loss: 0.722135\n",
            "epoch 31; iter: 0; batch classifier loss: 0.697152; batch adversarial loss: 0.722091\n",
            "epoch 32; iter: 0; batch classifier loss: 0.629634; batch adversarial loss: 0.696396\n",
            "epoch 33; iter: 0; batch classifier loss: 0.621336; batch adversarial loss: 0.717335\n",
            "epoch 34; iter: 0; batch classifier loss: 0.626697; batch adversarial loss: 0.698082\n",
            "epoch 35; iter: 0; batch classifier loss: 0.624217; batch adversarial loss: 0.718712\n",
            "epoch 36; iter: 0; batch classifier loss: 0.544514; batch adversarial loss: 0.717723\n",
            "epoch 37; iter: 0; batch classifier loss: 0.629370; batch adversarial loss: 0.689240\n",
            "epoch 38; iter: 0; batch classifier loss: 0.614702; batch adversarial loss: 0.698551\n",
            "epoch 39; iter: 0; batch classifier loss: 0.672410; batch adversarial loss: 0.710984\n",
            "epoch 40; iter: 0; batch classifier loss: 0.587630; batch adversarial loss: 0.714348\n",
            "epoch 41; iter: 0; batch classifier loss: 0.674753; batch adversarial loss: 0.703680\n",
            "epoch 42; iter: 0; batch classifier loss: 0.644672; batch adversarial loss: 0.666521\n",
            "epoch 43; iter: 0; batch classifier loss: 0.710662; batch adversarial loss: 0.680699\n",
            "epoch 44; iter: 0; batch classifier loss: 0.589836; batch adversarial loss: 0.694187\n",
            "epoch 45; iter: 0; batch classifier loss: 0.620084; batch adversarial loss: 0.659936\n",
            "epoch 46; iter: 0; batch classifier loss: 0.591313; batch adversarial loss: 0.701717\n",
            "epoch 47; iter: 0; batch classifier loss: 0.606023; batch adversarial loss: 0.663270\n",
            "epoch 48; iter: 0; batch classifier loss: 0.663075; batch adversarial loss: 0.676482\n",
            "epoch 49; iter: 0; batch classifier loss: 0.613744; batch adversarial loss: 0.714294\n",
            "epoch 50; iter: 0; batch classifier loss: 0.536268; batch adversarial loss: 0.699804\n",
            "epoch 51; iter: 0; batch classifier loss: 0.633456; batch adversarial loss: 0.685383\n",
            "epoch 52; iter: 0; batch classifier loss: 0.648048; batch adversarial loss: 0.667174\n",
            "epoch 53; iter: 0; batch classifier loss: 0.594320; batch adversarial loss: 0.675744\n",
            "epoch 54; iter: 0; batch classifier loss: 0.647735; batch adversarial loss: 0.706293\n",
            "epoch 55; iter: 0; batch classifier loss: 0.651845; batch adversarial loss: 0.666352\n",
            "epoch 56; iter: 0; batch classifier loss: 0.613156; batch adversarial loss: 0.689798\n",
            "epoch 57; iter: 0; batch classifier loss: 0.634903; batch adversarial loss: 0.657272\n",
            "epoch 58; iter: 0; batch classifier loss: 0.555088; batch adversarial loss: 0.680603\n",
            "epoch 59; iter: 0; batch classifier loss: 0.620390; batch adversarial loss: 0.689410\n",
            "epoch 60; iter: 0; batch classifier loss: 0.572764; batch adversarial loss: 0.668992\n",
            "epoch 61; iter: 0; batch classifier loss: 0.653044; batch adversarial loss: 0.661325\n",
            "epoch 62; iter: 0; batch classifier loss: 0.566566; batch adversarial loss: 0.666385\n",
            "epoch 63; iter: 0; batch classifier loss: 0.610377; batch adversarial loss: 0.661103\n",
            "epoch 64; iter: 0; batch classifier loss: 0.583911; batch adversarial loss: 0.670360\n",
            "epoch 65; iter: 0; batch classifier loss: 0.619934; batch adversarial loss: 0.685860\n",
            "epoch 66; iter: 0; batch classifier loss: 0.614654; batch adversarial loss: 0.632492\n",
            "epoch 67; iter: 0; batch classifier loss: 0.652580; batch adversarial loss: 0.651034\n",
            "epoch 68; iter: 0; batch classifier loss: 0.634027; batch adversarial loss: 0.667478\n",
            "epoch 69; iter: 0; batch classifier loss: 0.628470; batch adversarial loss: 0.674257\n",
            "epoch 70; iter: 0; batch classifier loss: 0.602921; batch adversarial loss: 0.672800\n",
            "epoch 71; iter: 0; batch classifier loss: 0.629076; batch adversarial loss: 0.671579\n",
            "epoch 72; iter: 0; batch classifier loss: 0.617413; batch adversarial loss: 0.666439\n",
            "epoch 73; iter: 0; batch classifier loss: 0.603703; batch adversarial loss: 0.678295\n",
            "epoch 74; iter: 0; batch classifier loss: 0.621133; batch adversarial loss: 0.691399\n",
            "epoch 75; iter: 0; batch classifier loss: 0.615215; batch adversarial loss: 0.660596\n",
            "epoch 76; iter: 0; batch classifier loss: 0.635053; batch adversarial loss: 0.652102\n",
            "epoch 77; iter: 0; batch classifier loss: 0.671298; batch adversarial loss: 0.698040\n",
            "epoch 78; iter: 0; batch classifier loss: 0.608160; batch adversarial loss: 0.680630\n",
            "epoch 79; iter: 0; batch classifier loss: 0.648270; batch adversarial loss: 0.635483\n",
            "epoch 80; iter: 0; batch classifier loss: 0.647318; batch adversarial loss: 0.677955\n",
            "epoch 81; iter: 0; batch classifier loss: 0.645299; batch adversarial loss: 0.721392\n",
            "epoch 82; iter: 0; batch classifier loss: 0.592336; batch adversarial loss: 0.681263\n",
            "epoch 83; iter: 0; batch classifier loss: 0.645669; batch adversarial loss: 0.671200\n",
            "epoch 84; iter: 0; batch classifier loss: 0.637491; batch adversarial loss: 0.668178\n",
            "epoch 85; iter: 0; batch classifier loss: 0.573975; batch adversarial loss: 0.679657\n",
            "epoch 86; iter: 0; batch classifier loss: 0.637417; batch adversarial loss: 0.650801\n",
            "epoch 87; iter: 0; batch classifier loss: 0.613643; batch adversarial loss: 0.661921\n",
            "epoch 88; iter: 0; batch classifier loss: 0.613756; batch adversarial loss: 0.658656\n",
            "epoch 89; iter: 0; batch classifier loss: 0.672378; batch adversarial loss: 0.680972\n",
            "epoch 90; iter: 0; batch classifier loss: 0.672777; batch adversarial loss: 0.679242\n",
            "epoch 91; iter: 0; batch classifier loss: 0.614985; batch adversarial loss: 0.670548\n",
            "epoch 92; iter: 0; batch classifier loss: 0.631097; batch adversarial loss: 0.650874\n",
            "epoch 93; iter: 0; batch classifier loss: 0.642512; batch adversarial loss: 0.669526\n",
            "epoch 94; iter: 0; batch classifier loss: 0.574519; batch adversarial loss: 0.651940\n",
            "epoch 95; iter: 0; batch classifier loss: 0.604544; batch adversarial loss: 0.679638\n",
            "epoch 96; iter: 0; batch classifier loss: 0.627907; batch adversarial loss: 0.665608\n",
            "epoch 97; iter: 0; batch classifier loss: 0.626718; batch adversarial loss: 0.678736\n",
            "epoch 98; iter: 0; batch classifier loss: 0.669004; batch adversarial loss: 0.663828\n",
            "epoch 99; iter: 0; batch classifier loss: 0.667963; batch adversarial loss: 0.684823\n",
            "epoch 100; iter: 0; batch classifier loss: 0.614928; batch adversarial loss: 0.642277\n",
            "epoch 101; iter: 0; batch classifier loss: 0.629539; batch adversarial loss: 0.657763\n",
            "epoch 102; iter: 0; batch classifier loss: 0.640994; batch adversarial loss: 0.657384\n",
            "epoch 103; iter: 0; batch classifier loss: 0.651661; batch adversarial loss: 0.636892\n",
            "epoch 104; iter: 0; batch classifier loss: 0.624735; batch adversarial loss: 0.656672\n",
            "epoch 105; iter: 0; batch classifier loss: 0.632298; batch adversarial loss: 0.644341\n",
            "epoch 106; iter: 0; batch classifier loss: 0.590322; batch adversarial loss: 0.630745\n",
            "epoch 107; iter: 0; batch classifier loss: 0.620976; batch adversarial loss: 0.652176\n",
            "epoch 108; iter: 0; batch classifier loss: 0.600754; batch adversarial loss: 0.664373\n",
            "epoch 109; iter: 0; batch classifier loss: 0.584041; batch adversarial loss: 0.632686\n",
            "epoch 110; iter: 0; batch classifier loss: 0.609875; batch adversarial loss: 0.680351\n",
            "epoch 111; iter: 0; batch classifier loss: 0.659242; batch adversarial loss: 0.677137\n",
            "epoch 112; iter: 0; batch classifier loss: 0.658161; batch adversarial loss: 0.678964\n",
            "epoch 113; iter: 0; batch classifier loss: 0.609828; batch adversarial loss: 0.649855\n",
            "epoch 114; iter: 0; batch classifier loss: 0.611498; batch adversarial loss: 0.652501\n",
            "epoch 115; iter: 0; batch classifier loss: 0.655681; batch adversarial loss: 0.675980\n",
            "epoch 116; iter: 0; batch classifier loss: 0.633288; batch adversarial loss: 0.671539\n",
            "epoch 117; iter: 0; batch classifier loss: 0.593429; batch adversarial loss: 0.674655\n",
            "epoch 118; iter: 0; batch classifier loss: 0.579473; batch adversarial loss: 0.660326\n",
            "epoch 119; iter: 0; batch classifier loss: 0.622802; batch adversarial loss: 0.667497\n",
            "epoch 120; iter: 0; batch classifier loss: 0.600817; batch adversarial loss: 0.667235\n",
            "epoch 121; iter: 0; batch classifier loss: 0.653716; batch adversarial loss: 0.679514\n",
            "epoch 122; iter: 0; batch classifier loss: 0.623451; batch adversarial loss: 0.668946\n",
            "epoch 123; iter: 0; batch classifier loss: 0.597050; batch adversarial loss: 0.653175\n",
            "epoch 124; iter: 0; batch classifier loss: 0.632954; batch adversarial loss: 0.666085\n",
            "epoch 125; iter: 0; batch classifier loss: 0.614618; batch adversarial loss: 0.669442\n",
            "epoch 126; iter: 0; batch classifier loss: 0.602974; batch adversarial loss: 0.678198\n",
            "epoch 127; iter: 0; batch classifier loss: 0.639270; batch adversarial loss: 0.646679\n",
            "epoch 128; iter: 0; batch classifier loss: 0.581186; batch adversarial loss: 0.658806\n",
            "epoch 129; iter: 0; batch classifier loss: 0.583194; batch adversarial loss: 0.687978\n",
            "epoch 130; iter: 0; batch classifier loss: 0.633670; batch adversarial loss: 0.714223\n",
            "epoch 131; iter: 0; batch classifier loss: 0.670928; batch adversarial loss: 0.680266\n",
            "epoch 132; iter: 0; batch classifier loss: 0.645923; batch adversarial loss: 0.656455\n",
            "epoch 133; iter: 0; batch classifier loss: 0.601402; batch adversarial loss: 0.655927\n",
            "epoch 134; iter: 0; batch classifier loss: 0.594243; batch adversarial loss: 0.681835\n",
            "epoch 135; iter: 0; batch classifier loss: 0.653446; batch adversarial loss: 0.657862\n",
            "epoch 136; iter: 0; batch classifier loss: 0.595753; batch adversarial loss: 0.676057\n",
            "epoch 137; iter: 0; batch classifier loss: 0.661050; batch adversarial loss: 0.662881\n",
            "epoch 138; iter: 0; batch classifier loss: 0.621708; batch adversarial loss: 0.660311\n",
            "epoch 139; iter: 0; batch classifier loss: 0.664450; batch adversarial loss: 0.645646\n",
            "epoch 140; iter: 0; batch classifier loss: 0.641450; batch adversarial loss: 0.657750\n",
            "epoch 141; iter: 0; batch classifier loss: 0.621327; batch adversarial loss: 0.673852\n",
            "epoch 142; iter: 0; batch classifier loss: 0.638428; batch adversarial loss: 0.671961\n",
            "epoch 143; iter: 0; batch classifier loss: 0.622311; batch adversarial loss: 0.687032\n",
            "epoch 144; iter: 0; batch classifier loss: 0.606286; batch adversarial loss: 0.690370\n",
            "epoch 145; iter: 0; batch classifier loss: 0.610546; batch adversarial loss: 0.668630\n",
            "epoch 146; iter: 0; batch classifier loss: 0.642308; batch adversarial loss: 0.683368\n",
            "epoch 147; iter: 0; batch classifier loss: 0.615993; batch adversarial loss: 0.677956\n",
            "epoch 148; iter: 0; batch classifier loss: 0.667431; batch adversarial loss: 0.639433\n",
            "epoch 149; iter: 0; batch classifier loss: 0.657471; batch adversarial loss: 0.646003\n",
            "epoch 150; iter: 0; batch classifier loss: 0.683269; batch adversarial loss: 0.684000\n",
            "epoch 151; iter: 0; batch classifier loss: 0.635624; batch adversarial loss: 0.672072\n",
            "epoch 152; iter: 0; batch classifier loss: 0.577819; batch adversarial loss: 0.644189\n",
            "epoch 153; iter: 0; batch classifier loss: 0.590658; batch adversarial loss: 0.659310\n",
            "epoch 154; iter: 0; batch classifier loss: 0.577788; batch adversarial loss: 0.654179\n",
            "epoch 155; iter: 0; batch classifier loss: 0.684258; batch adversarial loss: 0.662882\n",
            "epoch 156; iter: 0; batch classifier loss: 0.645426; batch adversarial loss: 0.697425\n",
            "epoch 157; iter: 0; batch classifier loss: 0.639898; batch adversarial loss: 0.657191\n",
            "epoch 158; iter: 0; batch classifier loss: 0.609423; batch adversarial loss: 0.641876\n",
            "epoch 159; iter: 0; batch classifier loss: 0.623936; batch adversarial loss: 0.666225\n",
            "epoch 160; iter: 0; batch classifier loss: 0.603911; batch adversarial loss: 0.673627\n",
            "epoch 161; iter: 0; batch classifier loss: 0.663594; batch adversarial loss: 0.674272\n",
            "epoch 162; iter: 0; batch classifier loss: 0.523039; batch adversarial loss: 0.667554\n",
            "epoch 163; iter: 0; batch classifier loss: 0.661693; batch adversarial loss: 0.679760\n",
            "epoch 164; iter: 0; batch classifier loss: 0.696951; batch adversarial loss: 0.674797\n",
            "epoch 165; iter: 0; batch classifier loss: 0.634771; batch adversarial loss: 0.667571\n",
            "epoch 166; iter: 0; batch classifier loss: 0.637057; batch adversarial loss: 0.655365\n",
            "epoch 167; iter: 0; batch classifier loss: 0.627913; batch adversarial loss: 0.648096\n",
            "epoch 168; iter: 0; batch classifier loss: 0.599689; batch adversarial loss: 0.700686\n",
            "epoch 169; iter: 0; batch classifier loss: 0.655143; batch adversarial loss: 0.642784\n",
            "epoch 170; iter: 0; batch classifier loss: 0.652630; batch adversarial loss: 0.689164\n",
            "epoch 171; iter: 0; batch classifier loss: 0.647703; batch adversarial loss: 0.668623\n",
            "epoch 172; iter: 0; batch classifier loss: 0.686852; batch adversarial loss: 0.667902\n",
            "epoch 173; iter: 0; batch classifier loss: 0.592964; batch adversarial loss: 0.703504\n",
            "epoch 174; iter: 0; batch classifier loss: 0.674280; batch adversarial loss: 0.673134\n",
            "epoch 175; iter: 0; batch classifier loss: 0.588669; batch adversarial loss: 0.660532\n",
            "epoch 176; iter: 0; batch classifier loss: 0.565077; batch adversarial loss: 0.630662\n",
            "epoch 177; iter: 0; batch classifier loss: 0.530621; batch adversarial loss: 0.684146\n",
            "epoch 178; iter: 0; batch classifier loss: 0.602222; batch adversarial loss: 0.681054\n",
            "epoch 179; iter: 0; batch classifier loss: 0.606314; batch adversarial loss: 0.696902\n",
            "epoch 180; iter: 0; batch classifier loss: 0.629619; batch adversarial loss: 0.642913\n",
            "epoch 181; iter: 0; batch classifier loss: 0.675138; batch adversarial loss: 0.644879\n",
            "epoch 182; iter: 0; batch classifier loss: 0.700016; batch adversarial loss: 0.664294\n",
            "epoch 183; iter: 0; batch classifier loss: 0.625423; batch adversarial loss: 0.689201\n",
            "epoch 184; iter: 0; batch classifier loss: 0.638734; batch adversarial loss: 0.671725\n",
            "epoch 185; iter: 0; batch classifier loss: 0.588182; batch adversarial loss: 0.701558\n",
            "epoch 186; iter: 0; batch classifier loss: 0.610445; batch adversarial loss: 0.684454\n",
            "epoch 187; iter: 0; batch classifier loss: 0.577426; batch adversarial loss: 0.675064\n",
            "epoch 188; iter: 0; batch classifier loss: 0.627915; batch adversarial loss: 0.668881\n",
            "epoch 189; iter: 0; batch classifier loss: 0.675220; batch adversarial loss: 0.655546\n",
            "epoch 190; iter: 0; batch classifier loss: 0.617933; batch adversarial loss: 0.635538\n",
            "epoch 191; iter: 0; batch classifier loss: 0.663969; batch adversarial loss: 0.656118\n",
            "epoch 192; iter: 0; batch classifier loss: 0.697128; batch adversarial loss: 0.645198\n",
            "epoch 193; iter: 0; batch classifier loss: 0.591437; batch adversarial loss: 0.711013\n",
            "epoch 194; iter: 0; batch classifier loss: 0.619404; batch adversarial loss: 0.661789\n",
            "epoch 195; iter: 0; batch classifier loss: 0.634755; batch adversarial loss: 0.675859\n",
            "epoch 196; iter: 0; batch classifier loss: 0.646680; batch adversarial loss: 0.645339\n",
            "epoch 197; iter: 0; batch classifier loss: 0.624403; batch adversarial loss: 0.645906\n",
            "epoch 198; iter: 0; batch classifier loss: 0.604734; batch adversarial loss: 0.669766\n",
            "epoch 199; iter: 0; batch classifier loss: 0.609448; batch adversarial loss: 0.661533\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x7f82c7afcaf0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "debiased_model.fit(orig_train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zGqHC8WyYy1N"
      },
      "outputs": [],
      "source": [
        "debiasing_train_data = debiased_model.predict(orig_train_set)\n",
        "debiasing_test_data = debiased_model.predict(orig_test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JwxWZ-NyYy1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03889c89-fa9b-46b2-c270-bac2035474a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plain model - without debiasing - dataset metrics\n",
            "Train set: Difference in mean outcomes between unprivileged and privileged groups =  -0.27483\n",
            "Test set: Difference in mean outcomes between unprivileged and privileged groups =  -0.26248\n",
            "\n",
            "Model - with debiasing - dataset metrics\n",
            "Train set: Difference in mean outcomes between unprivileged and privileged groups =  -0.0637\n",
            "Test set: Difference in mean outcomes between unprivileged and privileged groups =  -0.04676\n",
            "\n",
            "Plain model - without debiasing - classification metrics\n",
            "Test set: Classification accuracy =  0.68119\n",
            "Test set: Balanced classification accuracy =  0.67415\n",
            "Test set: Disparate impact =  0.65645\n",
            "Test set: Equal opportunity difference =  -0.17953\n",
            "Test set: Average odds difference =  -0.22444\n",
            "Test set: Theil_index =  0.18615\n",
            "\n",
            "Model - with debiasing - classification metrics\n",
            "Test set: Classification accuracy =  0.6648\n",
            "Test set: Balanced classification accuracy =  0.66155\n",
            "Test set: Disparate impact =  0.91974\n",
            "Test set: Equal opportunity difference =  0.02798\n",
            "Test set: Average odds difference =  -0.0068\n",
            "Test set: Theil_index =  0.22386\n"
          ]
        }
      ],
      "source": [
        "print(\"Plain model - without debiasing - dataset metrics\")\n",
        "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = \", float(round(nodebiasing_train_data_metric.mean_difference(),5)))\n",
        "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = \", float(round(nodebiasing_test_data_metric.mean_difference(),5)))\n",
        "\n",
        "print(\"\\nModel - with debiasing - dataset metrics\")\n",
        "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(debiasing_train_data, \n",
        "                                             unprivileged_groups=non_privileged,\n",
        "                                             privileged_groups=privileged)\n",
        "\n",
        "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = \", float(round(metric_dataset_debiasing_train.mean_difference(),5)))\n",
        "\n",
        "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(debiasing_test_data, \n",
        "                                             unprivileged_groups=non_privileged,\n",
        "                                             privileged_groups=privileged)\n",
        "\n",
        "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = \", float(round(metric_dataset_debiasing_test.mean_difference(),5)))\n",
        "\n",
        "print(\"\\nPlain model - without debiasing - classification metrics\")\n",
        "print(\"Test set: Classification accuracy = \", float(round(nodebiasing_test_classify_metric.accuracy(),5)))\n",
        "\n",
        "bal_acc_nodebiasing_test = 0.5*(nodebiasing_test_classify_metric.true_positive_rate()+nodebiasing_test_classify_metric.true_negative_rate())\n",
        "print(\"Test set: Balanced classification accuracy = \", float(round(bal_acc_nodebiasing_test,5)))\n",
        "print(\"Test set: Disparate impact = \",float(round(nodebiasing_test_classify_metric.disparate_impact(),5)))\n",
        "print(\"Test set: Equal opportunity difference = \", float(round(nodebiasing_test_classify_metric.equal_opportunity_difference(),5)))\n",
        "print(\"Test set: Average odds difference = \", float(round(nodebiasing_test_classify_metric.average_odds_difference(),5)))\n",
        "print(\"Test set: Theil_index = \", float(round(nodebiasing_test_classify_metric.theil_index(),5)))\n",
        "\n",
        "print(\"\\nModel - with debiasing - classification metrics\")\n",
        "classified_metric_debiasing_test = ClassificationMetric(orig_test_set, debiasing_test_data, unprivileged_groups=non_privileged, privileged_groups=privileged)\n",
        "print(\"Test set: Classification accuracy = \", float(round(classified_metric_debiasing_test.accuracy(),4)))\n",
        "\n",
        "bal_acc_debiasing_test = (classified_metric_debiasing_test.true_positive_rate()+classified_metric_debiasing_test.true_negative_rate())/2\n",
        "print(\"Test set: Balanced classification accuracy = \", float(round( bal_acc_debiasing_test,5)))\n",
        "print(\"Test set: Disparate impact = \", float(round(classified_metric_debiasing_test.disparate_impact(),5)))\n",
        "print(\"Test set: Equal opportunity difference = \", float(round(classified_metric_debiasing_test.equal_opportunity_difference(),5)))\n",
        "print(\"Test set: Average odds difference = \", float(round(classified_metric_debiasing_test.average_odds_difference(),5)))\n",
        "print(\"Test set: Theil_index = \", float(round(classified_metric_debiasing_test.theil_index(),5)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_comparison_graphs(X,y):\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    lst = [X.disparate_impact(),y.disparate_impact() ]\n",
        "    plt.bar(['before','after'],lst , width=0.2)\n",
        "    plt.ylim(-0.5,2)\n",
        "    plt.title('Disparate impact')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    lst=[X.equal_opportunity_difference(),y.equal_opportunity_difference() ]\n",
        "    plt.bar(['before','after'],lst , width=0.2)\n",
        "    plt.title('Equal opportunity difference')\n",
        "    plt.ylim(-0.5,2)\n",
        "    plt.show()\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    lst=[X.average_odds_difference(),y.average_odds_difference() ]\n",
        "    plt.bar(['before','after'],lst , width=0.2)\n",
        "    plt.ylim(-0.5,0.5)\n",
        "    plt.title('Average odds difference')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    lst=[X.theil_index(),y.theil_index()]\n",
        "    plt.bar(['before','after'],lst , width=0.2)\n",
        "    plt.title('Theil index')\n",
        "    plt.ylim(-0.25,0.25)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CkaXUVb8K-_o"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_comparison_graphs(nodebiasing_test_classify_metric, classified_metric_debiasing_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "W0wpL0Ywjw5b",
        "outputId": "d0ec2236-3636-410f-a71d-afbc1de57821"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HFqYHWczj4Ot"
      },
      "execution_count": 18,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}